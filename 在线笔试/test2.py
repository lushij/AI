import fitz  # PyMuPDF
import pytesseract
from PIL import Image, ImageEnhance, ImageFilter
import io
import re
import json
import os
import cv2
import numpy as np
from collections import defaultdict


class OptimizedComponentExtractor:
    def __init__(self, tesseract_path=None):
        """
        ä¼˜åŒ–ç‰ˆå…ƒå™¨ä»¶æå–å™¨
        """
        # è®¾ç½®Tesseractè·¯å¾„
        if tesseract_path and os.path.exists(tesseract_path):
            pytesseract.pytesseract.tesseract_cmd = tesseract_path
            print(f"âœ“ Tesseractè·¯å¾„: {tesseract_path}")
        else:
            # è‡ªåŠ¨æŸ¥æ‰¾
            auto_path = self._find_tesseract()
            if auto_path:
                pytesseract.pytesseract.tesseract_cmd = auto_path
                print(f"âœ“ è‡ªåŠ¨æ‰¾åˆ°Tesseract: {auto_path}")
            else:
                print("âŒ æœªæ‰¾åˆ°Tesseract")
                return

        # éªŒè¯Tesseract
        try:
            version = pytesseract.get_tesseract_version()
            print(f"âœ“ Tesseractç‰ˆæœ¬: {version}")
        except:
            print("âŒ Tesseractä¸å¯ç”¨")
            return

        # å…ƒå™¨ä»¶çŸ¥è¯†åº“ï¼ˆæ‰©å±•ç‰ˆï¼‰
        self.component_knowledge = {
            # æ˜¾ç¤ºè®¾å¤‡
            'display': {
                'keywords': ['æ¶²æ™¶å±', 'æ˜¾ç¤ºå±', 'æ˜¾ç¤ºå™¨', 'å±å¹•', 'LCD', 'LEDå±', 'è§¦æ‘¸å±', 'æ˜¾ç¤ºé¢æ¿'],
                'patterns': [r'æ¶²æ™¶å±', r'æ˜¾ç¤ºå±', r'æ˜¾ç¤º.*å±'],
                'description': 'æ˜¾ç¤ºè®¾å¤‡'
            },
            # ä¼ æ„Ÿå™¨
            'sensor': {
                'keywords': ['ä¼ æ„Ÿå™¨', 'æ¢å¤´', 'æ„Ÿåº”å™¨', 'æ£€æµ‹å™¨', 'æµ‹æ¸©', 'æµ‹å‹', 'æµ‹ä½'],
                'patterns': [r'ä¼ æ„Ÿå™¨', r'[æ¸©å‹ä½é€Ÿè½¬]ä¼ æ„Ÿå™¨', r'.*æ¢å¤´'],
                'description': 'ä¼ æ„Ÿå™¨ç±»'
            },
            # å¼€å…³
            'switch': {
                'keywords': ['å¼€å…³', 'æŒ‰é’®', 'æŒ‰é”®', 'æ—‹é’®', 'æ‹¨æ†', 'ç¿˜æ¿'],
                'patterns': [r'å¼€å…³', r'æŒ‰é’®', r'æŒ‰é”®', r'æ—‹é’®'],
                'description': 'å¼€å…³ç±»'
            },
            # è¿æ¥å™¨
            'connector': {
                'keywords': ['è¿æ¥å™¨', 'æ’å¤´', 'æ’åº§', 'ç«¯å­', 'æ¥æ’ä»¶', 'æ¥å¤´'],
                'patterns': [r'è¿æ¥å™¨', r'[æ’æ¥]å¤´', r'[æ’æ¥]åº§', r'ç«¯å­'],
                'description': 'è¿æ¥å™¨'
            },
            # æ§åˆ¶æ¨¡å—
            'controller': {
                'keywords': ['æ¨¡å—', 'ECU', 'ç”µè„‘', 'æ§åˆ¶å™¨', 'æ§åˆ¶å•å…ƒ', 'æ§åˆ¶æ¨¡å—'],
                'patterns': [r'[æ¨¡ç”µ]æ§', r'ECU', r'æ§åˆ¶.*æ¨¡å—', r'ç”µè„‘æ¿'],
                'description': 'æ§åˆ¶æ¨¡å—'
            },
            # çº¿æŸ
            'harness': {
                'keywords': ['çº¿æŸ', 'ç”µç¼†', 'ç”µçº¿', 'å¯¼çº¿', 'çº¿ç¼†', 'çº¿æŸæ€»æˆ'],
                'patterns': [r'çº¿æŸ', r'[ç”µçº¿]ç¼†', r'å¯¼çº¿'],
                'description': 'çº¿æŸç±»'
            },
            # ç‰¹æ®Šç³»ç»Ÿ
            'adblue': {
                'keywords': ['AdBlue', 'å°¿ç´ ', 'SCR', 'æ’æ”¾', 'å°¾æ°”'],
                'patterns': [r'AdBlue', r'å°¿ç´ ', r'SCR'],
                'description': 'å°¿ç´ ç³»ç»Ÿ'
            },
            # FA10å‘åŠ¨æœº
            'fa10': {
                'keywords': ['FA10', 'é”¡æŸ´', 'å‘åŠ¨æœº', 'å¼•æ“'],
                'patterns': [r'FA10', r'é”¡æŸ´', r'å‘åŠ¨æœº'],
                'description': 'FA10å‘åŠ¨æœº'
            }
        }

        # å°ºå¯¸/è§„æ ¼æ¨¡å¼
        self.spec_patterns = [
            r'(\d+\.?\d*)\s*[Ã—xX*]\s*(\d+\.?\d*)',  # 100Ã—50
            r'(\d+\.?\d*)\s*[Â±Â±]\s*(\d+\.?\d*)',  # 330.8Â±0.5
            r'[Î¦Ï•Ã˜]\s*(\d+\.?\d*)',  # Î¦10
            r'(\d+\.?\d*)\s*[Mm][Mm]',  # 100mm
            r'(\d+\.?\d*)\s*[Cc][Mm]',  # 10cm
            r'(\d+\.?\d*)\s*Â°[Cc]',  # 25Â°C
        ]

        # è¿æ¥å™¨ç¼–ç æ¨¡å¼
        self.connector_patterns = [
            r'([A-Z][0-9]+)P([0-9]+)',  # C2P1
            r'([A-Z][0-9]+)[-_\.]([0-9]+)',  # C2-1, C2_1, C2.1
            r'(J[0-9]+)',  # J100
            r'(X[0-9]+)',  # X1
        ]

        # å­˜å‚¨ç»“æœ
        self.components = []
        self.stats = defaultdict(int)

    def _find_tesseract(self):
        """æŸ¥æ‰¾Tesseract"""
        paths = [
            r'C:\Program Files\Tesseract-OCR\tesseract.exe',
            r'C:\Program Files (x86)\Tesseract-OCR\tesseract.exe',
            r'D:\Tesseract-OCR\tesseract.exe',
            r'E:\Tesseract-OCR\tesseract.exe',
            r'R:\Program Files (x86)\Tesseract-OCR\tesseract.exe',
        ]
        for path in paths:
            if os.path.exists(path):
                return path
        return None

    def _preprocess_image(self, image):
        """
        å›¾åƒé¢„å¤„ç† - æé«˜OCRå‡†ç¡®ç‡
        """
        # è½¬æ¢ä¸ºç°åº¦
        if image.mode != 'L':
            gray = image.convert('L')
        else:
            gray = image

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        img_array = np.array(gray)

        # 1. è‡ªé€‚åº”é˜ˆå€¼ï¼ˆæé«˜å¯¹æ¯”åº¦ï¼‰
        binary = cv2.adaptiveThreshold(img_array, 255,
                                       cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                       cv2.THRESH_BINARY, 11, 2)

        # 2. é™å™ª
        denoised = cv2.medianBlur(binary, 3)

        # 3. é”åŒ–ï¼ˆå¢å¼ºè¾¹ç¼˜ï¼‰
        kernel = np.array([[0, -1, 0],
                           [-1, 5, -1],
                           [0, -1, 0]])
        sharpened = cv2.filter2D(denoised, -1, kernel)

        # 4. å½¢æ€å­¦æ“ä½œï¼ˆè¿æ¥æ–­å¼€çš„ç¬”ç”»ï¼‰
        kernel2 = np.ones((2, 2), np.uint8)
        morphed = cv2.morphologyEx(sharpened, cv2.MORPH_CLOSE, kernel2)

        # è½¬å›PILå›¾åƒ
        processed = Image.fromarray(morphed)

        return processed

    def _enhance_ocr_accuracy(self, image):
        """
        é¢å¤–çš„å›¾åƒå¢å¼º
        """
        # è°ƒæ•´å¯¹æ¯”åº¦
        enhancer = ImageEnhance.Contrast(image)
        image = enhancer.enhance(2.0)

        # è°ƒæ•´äº®åº¦
        enhancer = ImageEnhance.Brightness(image)
        image = enhancer.enhance(1.1)

        # é”åŒ–
        enhancer = ImageEnhance.Sharpness(image)
        image = enhancer.enhance(1.5)

        return image

    def _ocr_with_retry(self, image):
        """
        å¸¦é‡è¯•æœºåˆ¶çš„OCR
        """
        results = []

        # å°è¯•ä¸åŒçš„OCRé…ç½®
        configs = [
            {'lang': 'chi_sim', 'config': '--oem 3 --psm 6'},
            {'lang': 'chi_sim+eng', 'config': '--oem 3 --psm 6'},
            {'lang': 'eng', 'config': '--oem 3 --psm 6'},
            {'lang': 'chi_sim', 'config': '--oem 3 --psm 3'},
            {'lang': 'chi_sim', 'config': '--oem 1 --psm 6'},
        ]

        for config in configs:
            try:
                text = pytesseract.image_to_string(
                    image,
                    lang=config['lang'],
                    config=config['config']
                )

                if text and text.strip():
                    results.append({
                        'text': text,
                        'config': config,
                        'length': len(text.strip())
                    })

                    # å¦‚æœè¯†åˆ«ç»“æœè¶³å¤Ÿé•¿ï¼Œæå‰è¿”å›
                    if len(text.strip()) > 20:
                        break

            except Exception as e:
                continue

        # é€‰æ‹©æœ€å¥½çš„ç»“æœ
        if results:
            # æŒ‰æ–‡æœ¬é•¿åº¦æ’åº
            results.sort(key=lambda x: x['length'], reverse=True)
            return results[0]['text']

        return ""

    def extract_from_pdf(self, pdf_path, max_pages=None, save_images=False):
        """
        ä»PDFæå–å…ƒå™¨ä»¶
        """
        print(f"ğŸ” å¼€å§‹åˆ†æ: {os.path.basename(pdf_path)}")

        if not os.path.exists(pdf_path):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
            return []

        # åˆ›å»ºè¾“å‡ºç›®å½•
        if save_images:
            output_dir = "processed_images"
            os.makedirs(output_dir, exist_ok=True)

        try:
            # æ‰“å¼€PDF
            doc = fitz.open(pdf_path)
            total_pages = len(doc)
            if max_pages:
                total_pages = min(total_pages, max_pages)

            print(f"ğŸ“Š PDFæ€»é¡µæ•°: {len(doc)} (å¤„ç†å‰ {total_pages} é¡µ)")

            for page_num in range(total_pages):
                print(f"\nğŸ“– ç¬¬ {page_num + 1}/{total_pages} é¡µ")
                page = doc[page_num]

                # è·å–é¡µé¢å›¾åƒ
                image_list = page.get_images()

                if image_list:
                    print(f"  å‘ç° {len(image_list)} ä¸ªå›¾åƒ")
                    self.stats['total_images'] += len(image_list)

                    for img_idx, img_info in enumerate(image_list):
                        self.stats['processed_images'] += 1

                        try:
                            # æå–å›¾åƒ
                            xref = img_info[0]
                            base_image = doc.extract_image(xref)
                            image_bytes = base_image["image"]

                            # è½¬æ¢ä¸ºPILå›¾åƒ
                            original = Image.open(io.BytesIO(image_bytes))

                            # é¢„å¤„ç†å›¾åƒ
                            processed = self._preprocess_image(original)
                            enhanced = self._enhance_ocr_accuracy(processed)

                            # ä¿å­˜å¤„ç†åçš„å›¾åƒï¼ˆè°ƒè¯•ç”¨ï¼‰
                            if save_images:
                                img_name = f"page_{page_num + 1}_img_{img_idx + 1}.png"
                                enhanced.save(os.path.join(output_dir, img_name))

                            # OCRè¯†åˆ«
                            text = self._ocr_with_retry(enhanced)

                            if text and text.strip():
                                # æ¸…ç†æ–‡æœ¬
                                cleaned = self._clean_ocr_text(text)

                                print(f"    å›¾åƒ {img_idx + 1}: âœ“ è¯†åˆ«æˆåŠŸ ({len(cleaned)} å­—ç¬¦)")

                                # æå–å…ƒå™¨ä»¶
                                found = self._analyze_text(cleaned, page_num + 1, img_idx + 1)

                                if found:
                                    self.components.extend(found)
                                    print(f"      âœ… æ‰¾åˆ° {len(found)} ä¸ªå…ƒå™¨ä»¶")

                                    # æ˜¾ç¤ºå‰å‡ ä¸ª
                                    for comp in found[:3]:
                                        print(f"        â€¢ {comp['name']}")

                                # ä¿å­˜è¯†åˆ«çš„æ–‡æœ¬
                                if save_images and cleaned:
                                    txt_name = f"page_{page_num + 1}_img_{img_idx + 1}.txt"
                                    with open(os.path.join(output_dir, txt_name),
                                              'w', encoding='utf-8') as f:
                                        f.write(cleaned)

                            else:
                                print(f"    å›¾åƒ {img_idx + 1}: âš ï¸ æœªè¯†åˆ«åˆ°æ–‡å­—")

                        except Exception as e:
                            print(f"    å›¾åƒ {img_idx + 1}: âŒ å¤„ç†å¤±è´¥ - {str(e)[:50]}")

                else:
                    print(f"  âš ï¸ æœ¬é¡µæ— å›¾åƒ")

            doc.close()

        except Exception as e:
            print(f"âŒ å¤„ç†PDFå¤±è´¥: {e}")

        return self.components

    def _clean_ocr_text(self, text):
        """
        æ¸…ç†OCRè¯†åˆ«çš„æ–‡æœ¬
        """
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)

        # ä¿®å¤å¸¸è§OCRé”™è¯¯
        corrections = {
            'åœŸ': 'Â±',
            'ä¸€': '-',
            'ä¸‰': '=',
            'å·³': 'å·²',
            'æ›°': 'æ—¥',
            'å†«': 'å†°',
            'å': '+',
            'å£': 'å£',
        }

        for wrong, correct in corrections.items():
            text = text.replace(wrong, correct)

        # ä¿®å¤å°ºå¯¸ç¬¦å·
        text = re.sub(r'(\d+\.?\d*)\s*[å+]', r'\1Â±', text)

        return text.strip()

    def _analyze_text(self, text, page_num, img_num):
        """
        åˆ†ææ–‡æœ¬ï¼Œæå–å…ƒå™¨ä»¶ä¿¡æ¯
        """
        components = []

        # æŒ‰è¡Œåˆ†å‰²
        lines = text.split('\n')

        for line_num, line in enumerate(lines, 1):
            line = line.strip()
            if not line or len(line) < 2:
                continue

            # 1. æŸ¥æ‰¾å…ƒå™¨ä»¶å…³é”®è¯
            for category, info in self.component_knowledge.items():
                for keyword in info['keywords']:
                    if keyword in line:
                        component = self._create_component(
                            line, keyword, info['description'],
                            page_num, img_num, line_num
                        )
                        if component:
                            components.append(component)
                        break  # æ¯ä¸ªå…³é”®è¯åªåŒ¹é…ä¸€æ¬¡

            # 2. æŸ¥æ‰¾è¿æ¥å™¨ç¼–ç 
            for pattern in self.connector_patterns:
                matches = re.finditer(pattern, line)
                for match in matches:
                    if len(match.groups()) >= 1:
                        component = self._create_connector_component(
                            match, line, page_num, img_num, line_num
                        )
                        if component:
                            components.append(component)

            # 3. æŸ¥æ‰¾å°ºå¯¸è§„æ ¼
            for pattern in self.spec_patterns:
                matches = re.finditer(pattern, line)
                for match in matches:
                    component = self._create_spec_component(
                        match, line, page_num, img_num, line_num
                    )
                    if component:
                        components.append(component)

        return components

    def _create_component(self, line, keyword, category, page_num, img_num, line_num):
        """
        åˆ›å»ºå…ƒå™¨ä»¶ä¿¡æ¯
        """
        # æå–åŒ…å«å…³é”®è¯çš„ä¸Šä¸‹æ–‡
        name = self._extract_context(line, keyword)

        # æå–è§„æ ¼
        specs = self._extract_specifications(line)

        component = {
            'name': name,
            'category': category,
            'keyword': keyword,
            'page': page_num,
            'image': img_num,
            'line': line_num,
            'text': line[:100],
            'specifications': specs,
            'confidence': self._estimate_confidence(line, keyword),
            'type': 'keyword'
        }

        return component

    def _create_connector_component(self, match, line, page_num, img_num, line_num):
        """
        åˆ›å»ºè¿æ¥å™¨ç»„ä»¶
        """
        groups = match.groups()

        if len(groups) >= 2:
            # æ ¼å¼å¦‚ C2P1
            connector = groups[0]
            pin = groups[1]
            code = f"{connector}P{pin}"
            name = f"{connector}è¿æ¥å™¨ (é’ˆè„š{pin})"
        else:
            # æ ¼å¼å¦‚ J100
            connector = groups[0]
            pin = None
            code = connector
            name = f"{connector}è¿æ¥å™¨"

        component = {
            'name': name,
            'category': 'è¿æ¥å™¨',
            'connector': connector,
            'pin': pin,
            'code': code,
            'page': page_num,
            'image': img_num,
            'line': line_num,
            'text': line[:100],
            'confidence': 'é«˜',
            'type': 'connector'
        }

        return component

    def _create_spec_component(self, match, line, page_num, img_num, line_num):
        """
        åˆ›å»ºè§„æ ¼ç»„ä»¶
        """
        spec_text = match.group()

        component = {
            'name': f"è§„æ ¼: {spec_text}",
            'category': 'è§„æ ¼å‚æ•°',
            'specification': spec_text,
            'page': page_num,
            'image': img_num,
            'line': line_num,
            'text': line[:100],
            'confidence': 'é«˜',
            'type': 'specification'
        }

        return component

    def _extract_context(self, line, keyword):
        """
        æå–åŒ…å«å…³é”®è¯çš„ä¸Šä¸‹æ–‡
        """
        words = line.split()
        for i, word in enumerate(words):
            if keyword in word:
                # è·å–å‰åè¯æ±‡
                start = max(0, i - 2)
                end = min(len(words), i + 3)
                return ' '.join(words[start:end])

        return line[:50]

    def _extract_specifications(self, line):
        """
        æå–è§„æ ¼å‚æ•°
        """
        specs = {}

        for pattern in self.spec_patterns:
            matches = re.findall(pattern, line)
            if matches:
                if 'Ã—' in pattern or 'x' in pattern or 'X' in pattern:
                    specs['dimensions'] = matches[0]
                elif 'Â±' in pattern:
                    specs['tolerance'] = matches[0]
                elif 'Î¦' in pattern or 'Ï•' in pattern or 'Ã˜' in pattern:
                    specs['diameter'] = matches[0]
                elif 'mm' in pattern.lower():
                    specs['length_mm'] = matches
                elif 'cm' in pattern.lower():
                    specs['length_cm'] = matches
                elif 'Â°C' in pattern:
                    specs['temperature'] = matches

                # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå°±åœæ­¢
                break

        return specs

    def _estimate_confidence(self, line, keyword):
        """
        ä¼°è®¡è¯†åˆ«ç½®ä¿¡åº¦
        """
        if len(line) < 30 and keyword in line:
            return 'é«˜'
        elif len(line) < 100 and keyword in line:
            return 'ä¸­'
        else:
            return 'ä½'

    def generate_report(self, output_file="ocr_components_report.txt"):
        """
        ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        """
        print(f"\nğŸ“Š ç”Ÿæˆåˆ†ææŠ¥å‘Š...")

        report = []
        report.append("=" * 80)
        report.append("ğŸ“„ PDFå›¾åƒå…ƒå™¨ä»¶è¯†åˆ«æŠ¥å‘Š (ä½¿ç”¨OCR)")
        report.append("=" * 80)

        # ç»Ÿè®¡ä¿¡æ¯
        report.append(f"\nğŸ“ˆ å¤„ç†ç»Ÿè®¡:")
        report.append(f"  å¤„ç†çš„å›¾åƒæ€»æ•°: {self.stats.get('processed_images', 0)}")
        report.append(f"  æ‰¾åˆ°çš„å…ƒå™¨ä»¶æ€»æ•°: {len(self.components)}")

        if self.components:
            # æŒ‰ç±»åˆ«ç»Ÿè®¡
            category_stats = defaultdict(int)
            type_stats = defaultdict(int)
            confidence_stats = defaultdict(int)

            for comp in self.components:
                category_stats[comp['category']] += 1
                type_stats[comp['type']] += 1
                confidence_stats[comp['confidence']] += 1

            report.append(f"\nğŸ”§ å…ƒå™¨ä»¶åˆ†ç±»:")
            for category, count in sorted(category_stats.items(), key=lambda x: x[1], reverse=True):
                report.append(f"  {category}: {count}ä¸ª")

            report.append(f"\nğŸ“‹ è¯†åˆ«ç±»å‹:")
            for type_name, count in sorted(type_stats.items(), key=lambda x: x[1], reverse=True):
                report.append(f"  {type_name}: {count}ä¸ª")

            report.append(f"\nğŸ¯ ç½®ä¿¡åº¦åˆ†å¸ƒ:")
            for conf, count in sorted(confidence_stats.items()):
                icon = 'âœ…' if conf == 'é«˜' else 'âš ï¸' if conf == 'ä¸­' else 'â“'
                report.append(f"  {icon} {conf}: {count}ä¸ª")

            # è¯¦ç»†åˆ—è¡¨ï¼ˆæŒ‰é¡µç ï¼‰
            report.append(f"\nğŸ“– è¯¦ç»†å…ƒå™¨ä»¶åˆ—è¡¨:")

            by_page = defaultdict(list)
            for comp in self.components:
                by_page[comp['page']].append(comp)

            for page in sorted(by_page.keys()):
                report.append(f"\n  ç¬¬ {page} é¡µ ({len(by_page[page])}ä¸ª):")

                for i, comp in enumerate(by_page[page][:15], 1):
                    conf_icon = 'âœ…' if comp['confidence'] == 'é«˜' else 'âš ï¸' if comp['confidence'] == 'ä¸­' else 'â“'

                    line = f"    {i:2d}. {conf_icon} {comp['name']}"

                    if comp.get('code'):
                        line += f" [ç¼–ç : {comp['code']}]"

                    if comp.get('specifications'):
                        specs = comp['specifications']
                        if 'dimensions' in specs:
                            dims = specs['dimensions']
                            if isinstance(dims, tuple):
                                line += f" (å°ºå¯¸: {'Ã—'.join(map(str, dims))})"
                            else:
                                line += f" (å°ºå¯¸: {dims})"

                    report.append(line)

                if len(by_page[page]) > 15:
                    report.append(f"    ... è¿˜æœ‰ {len(by_page[page]) - 15} ä¸ª")

        else:
            report.append(f"\nâš ï¸ æœªæ‰¾åˆ°ä»»ä½•å…ƒå™¨ä»¶")

        report.append(f"\n{'=' * 80}")
        report.append("æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        report.append("=" * 80)

        report_text = "\n".join(report)

        # ä¿å­˜æŠ¥å‘Š
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report_text)
            print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")

        # æ‰“å°åˆ°æ§åˆ¶å°
        print(report_text)

        return report_text

    def export_data(self, output_file="ocr_components_data.json"):
        """
        å¯¼å‡ºæ•°æ®
        """
        if not self.components:
            print("æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
            return

        data = {
            'metadata': {
                'total_components': len(self.components),
                'processed_images': self.stats.get('processed_images', 0),
                'tesseract_version': '5.3.1.20230401',
                'analysis_method': 'ocr_based'
            },
            'components': self.components,
            'statistics': dict(self.stats)
        }

        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"âœ… æ•°æ®å·²å¯¼å‡ºåˆ°: {output_file}")
        except Exception as e:
            print(f"âŒ å¯¼å‡ºæ•°æ®å¤±è´¥: {e}")


def main():
    """
    ä¸»ç¨‹åº
    """
    print("=" * 80)
    print("ğŸš— æ±½è½¦çº¿æŸå›¾å…ƒå™¨ä»¶OCRè¯†åˆ«ç³»ç»Ÿ")
    print("=" * 80)
    print("è¯´æ˜: ä½¿ç”¨æ–°ç‰ˆTesseract 5.xè¿›è¡ŒOCRè¯†åˆ«")
    print("=" * 80)

    # æ£€æŸ¥ä¾èµ–
    try:
        import fitz
        import pytesseract
        from PIL import Image
        import cv2
        import numpy as np
        print("âœ“ æ‰€æœ‰ä¾èµ–åº“å·²å®‰è£…")
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–åº“: {e}")
        print("\nè¯·å®‰è£…:")
        print("pip install PyMuPDF pytesseract pillow opencv-python numpy")
        return

    # æŸ¥æ‰¾PDFæ–‡ä»¶
    pdf_files = [
        "test.pdf",
        "ä¸€æ±½è§£æ”¾_æ–°æ¬¾J6L_æ•´è½¦çº¿æŸå›¾ã€å«å„ç±»é€‰è£…é…ç½®ã€‘ã€é”¡æŸ´è‡ªä¸»FA10-æ°”é©±ç½ã€‘ã€å›½å…­ã€‘.pdf",
        "çº¿æŸå›¾.pdf",
        "harness.pdf",
    ]

    pdf_path = None
    for file in pdf_files:
        if os.path.exists(file):
            pdf_path = file
            print(f"âœ“ æ‰¾åˆ°PDFæ–‡ä»¶: {file}")
            break

    if not pdf_path:
        # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªPDF
        for file in os.listdir('.'):
            if file.lower().endswith('.pdf'):
                pdf_path = file
                print(f"âœ“ æ‰¾åˆ°PDFæ–‡ä»¶: {file}")
                break

    if not pdf_path:
        print("âŒ æœªæ‰¾åˆ°PDFæ–‡ä»¶")
        print("å½“å‰ç›®å½•:", os.listdir('.'))
        return

    # åˆ›å»ºæå–å™¨
    extractor = OptimizedComponentExtractor(
        tesseract_path=r'C:\Program Files\Tesseract-OCR\tesseract.exe'
    )

    # æå–å…ƒå™¨ä»¶
    print("\n" + "=" * 80)
    print("å¼€å§‹OCRè¯†åˆ«å…ƒå™¨ä»¶...")
    print("=" * 80)
    print("æ³¨æ„: é¦–æ¬¡è¿è¡Œå¯èƒ½è¾ƒæ…¢ï¼Œæ­£åœ¨å¤„ç†å›¾åƒ...")

    # å¤„ç†PDFï¼ˆå¯ä»¥é™åˆ¶é¡µæ•°è¿›è¡Œæµ‹è¯•ï¼‰
    components = extractor.extract_from_pdf(
        pdf_path=pdf_path,
        max_pages=None,  # Noneè¡¨ç¤ºå¤„ç†æ‰€æœ‰é¡µï¼Œå¯ä»¥è®¾ä¸º5è¿›è¡Œæµ‹è¯•
        save_images=True  # ä¿å­˜å¤„ç†åçš„å›¾åƒç”¨äºè°ƒè¯•
    )

    # ç”ŸæˆæŠ¥å‘Š
    print("\n" + "=" * 80)
    print("ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š...")
    print("=" * 80)

    extractor.generate_report("å…ƒå™¨ä»¶OCRè¯†åˆ«æŠ¥å‘Š.txt")

    # å¯¼å‡ºæ•°æ®
    extractor.export_data("å…ƒå™¨ä»¶OCRæ•°æ®.json")

    # æ˜¾ç¤ºå…³é”®ç»“æœ
    if components:
        print("\nğŸ¯ å…³é”®è¯†åˆ«ç»“æœ:")
        print("-" * 40)

        # é«˜ç½®ä¿¡åº¦ç»“æœ
        high_conf = [c for c in components if c['confidence'] == 'é«˜']
        if high_conf:
            print(f"\nâœ… é«˜ç½®ä¿¡åº¦å…ƒå™¨ä»¶ ({len(high_conf)}ä¸ª):")
            for comp in high_conf[:10]:
                print(f"  â€¢ {comp['name']}")
                if comp.get('page'):
                    print(f"    æ‰€åœ¨é¡µ: ç¬¬{comp['page']}é¡µ")
                if comp.get('specifications'):
                    specs = comp['specifications']
                    if 'dimensions' in specs:
                        print(f"    è§„æ ¼: {specs['dimensions']}")

        # è¿æ¥å™¨ä¿¡æ¯
        connectors = [c for c in components if c['type'] == 'connector']
        if connectors:
            print(f"\nğŸ”Œ è¿æ¥å™¨è¯†åˆ« ({len(connectors)}ä¸ª):")
            conn_stats = {}
            for conn in connectors:
                code = conn.get('connector')
                if code:
                    conn_stats[code] = conn_stats.get(code, 0) + 1

            for code, count in sorted(conn_stats.items()):
                print(f"  {code}ç³»åˆ—: {count}ä¸ªé’ˆè„š")

        # è§„æ ¼ä¿¡æ¯
        specs = [c for c in components if c['type'] == 'specification']
        if specs:
            print(f"\nğŸ“ å°ºå¯¸è§„æ ¼ ({len(specs)}ä¸ª):")
            for spec in specs[:5]:
                print(f"  {spec['specification']}")

    print(f"\nâœ¨ OCRè¯†åˆ«å®Œæˆï¼")


if __name__ == "__main__":
    main()